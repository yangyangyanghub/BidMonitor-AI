"""
ç›‘æ§æ ¸å¿ƒæ¨¡å— - æ•´åˆçˆ¬è™«ã€åŒ¹é…ã€é€šçŸ¥åŠŸèƒ½
"""
import logging
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from .database.storage import Storage, BidInfo
    from .matcher.keyword import KeywordMatcher
    from .notifier.email import EmailNotifier
    from .notifier.sms import SMSNotifier
    
    from .crawler.ccgp import CCGPCrawler
    from .crawler.chinabidding import ChinaBiddingCrawler
    from .crawler.ebnew import EbnewCrawler
    from .crawler.plap import PLAPCrawler
    from .crawler.ggzy import GGZYCrawler
    from .crawler.bidcenter import BidcenterCrawler
    from .crawler.qianlima import QianlimaCrawler
    from .crawler.chinatender import ChinaTenderCrawler
    from .crawler.solarbe import SolarbeCrawler
    from .crawler.pvyuan import PvyuanCrawler
    from .crawler.dlnyzb import DlnyzbCrawler
    from .crawler.youuav import YouuavCrawler
except ImportError:
    from database.storage import Storage, BidInfo
    from matcher.keyword import KeywordMatcher
    from notifier.email import EmailNotifier
    from notifier.sms import SMSNotifier
    
    from crawler.ccgp import CCGPCrawler
    from crawler.chinabidding import ChinaBiddingCrawler
    from crawler.ebnew import EbnewCrawler
    from crawler.plap import PLAPCrawler
    from crawler.ggzy import GGZYCrawler
    from crawler.bidcenter import BidcenterCrawler
    from crawler.qianlima import QianlimaCrawler
    from crawler.chinatender import ChinaTenderCrawler
    from crawler.solarbe import SolarbeCrawler
    from crawler.pvyuan import PvyuanCrawler
    from crawler.dlnyzb import DlnyzbCrawler
    from crawler.youuav import YouuavCrawler

# çˆ¬è™«æ³¨å†Œè¡¨
def get_all_crawlers():
    """è·å–æ‰€æœ‰çˆ¬è™«ç±»"""
    return {
        'chinabidding': ChinaBiddingCrawler,
        'ccgp': CCGPCrawler,
    }

# é»˜è®¤å†…ç½®ç½‘ç«™é…ç½® (ç”¨äºé€šç”¨çˆ¬è™«)
def get_default_sites():
    """è·å–é»˜è®¤çš„å†…ç½®ç½‘ç«™åˆ—è¡¨"""
    return {
        'chinabidding': {'name': 'ä¸­å›½é‡‡è´­ä¸æ‹›æ ‡ç½‘', 'url': 'http://www.chinabidding.cn/'},
        'dlzb': {'name': 'ä¸­å›½ç”µåŠ›æ‹›æ ‡ç½‘', 'url': 'http://www.dlzb.com/'},
        'chinabiddingcc': {'name': 'ä¸­å›½é‡‡è´­æ‹›æ ‡ç½‘', 'url': 'http://www.chinabidding.cc/'},
        'gdtzb': {'name': 'å›½ç”µæŠ•æ‹›æ ‡ç½‘', 'url': 'http://www.gdtzb.com'},
        'cpeinet': {'name': 'ä¸­å›½ç”µåŠ›è®¾å¤‡ä¿¡æ¯ç½‘', 'url': 'http://www.cpeinet.com.cn/'},
        'espic': {'name': 'ç”µèƒ½eæ‹›é‡‡', 'url': 'https://ebid.espic.com.cn/'},
        'chng': {'name': 'åèƒ½é›†å›¢ç”µå­å•†åŠ¡å¹³å°', 'url': 'http://ec.chng.com.cn/ecmall/'},
        'powerchina': {'name': 'ä¸­å›½ç”µå»ºé‡‡è´­ç”µå­å•†åŠ¡å¹³å°', 'url': 'http://ec.powerchina.cn'},
        'powerchina_bid': {'name': 'ä¸­å›½ç”µå»ºé‡‡è´­æ‹›æ ‡æ•°æ™ºåŒ–å¹³å°', 'url': 'https://bid.powerchina.cn/bidweb/'},
        'powerchina_ec': {'name': 'ä¸­å›½ç”µå»ºè®¾å¤‡ç‰©èµ„é›†ä¸­é‡‡è´­å¹³å°', 'url': 'https://ec.powerchina.cn/'},
        'powerchina_scm': {'name': 'ä¸­å›½ç”µå»ºä¾›åº”é“¾äº‘æœåŠ¡å¹³å°', 'url': 'https://scm.powerchina.cn/'},
        'powerchina_idx': {'name': 'ä¸­å›½ç”µå»ºæ‰¿åŒ…å•†ç®¡ç†ç³»ç»Ÿ', 'url': 'http://bid.powerchina.cn/index'},
        'powerchina_nw': {'name': 'ä¸­å›½ç”µå»ºè¥¿åŒ—å‹˜æµ‹è®¾è®¡ç ”ç©¶é™¢', 'url': 'http://ec1.powerchina.cn'},
        'ceec': {'name': 'ä¸­å›½èƒ½å»ºç”µå­é‡‡è´­å¹³å°', 'url': 'https://ec.ceec.net.cn/'},
        'chdtp': {'name': 'ä¸­å›½åç”µç”µå­å•†åŠ¡å¹³å°', 'url': 'http://www.chdtp.com/'},
        'chec_gys': {'name': 'ä¸­å›½åç”µç§‘å·¥ä¾›åº”å•†å¡«æŠ¥ç³»ç»Ÿ', 'url': 'http://gys.chec.com.cn:90'},
        'chinazbcg': {'name': 'ä¸­å›½æ‹›æŠ•æ ‡ä¿¡æ¯ç½‘', 'url': 'http://www.chinazbcg.com'},
        'cdt': {'name': 'ä¸­å›½å¤§å”ç”µå­å•†åŠ¡å¹³å°', 'url': 'http://www.cdt-ec.com/'},
        'ebidding': {'name': 'å›½ä¹‰æ‹›æ ‡', 'url': 'http://www.ebidding.com/portal/'},
        'neep': {'name': 'å›½å®¶èƒ½æºeè´­', 'url': 'https://www.neep.shop/'},
        'ceic': {'name': 'å›½å®¶èƒ½æºé›†å›¢ç”Ÿæ€åä½œå¹³å°', 'url': 'https://cooperation.ceic.com/'},
        'sgcc': {'name': 'å›½å®¶ç”µç½‘ç”µå­å•†åŠ¡å¹³å°', 'url': 'https://ecp.sgcc.com.cn/'},
        'cecep': {'name': 'ä¸­å›½èŠ‚èƒ½ç¯ä¿ç”µå­é‡‡è´­å¹³å°', 'url': 'http://www.ebidding.cecep.cn/'},
        'gdg': {'name': 'å¹¿å·å‘å±•é›†å›¢ç”µå­é‡‡è´­å¹³å°', 'url': 'https://eps.gdg.com.cn/'},
        'crpower': {'name': 'åæ¶¦ç”µåŠ›', 'url': 'https://b2b.crpower.com.cn'},
        'crc': {'name': 'åæ¶¦é›†å›¢å®ˆæ­£ç”µå­æ‹›æ ‡é‡‡è´­å¹³å°', 'url': 'https://szecp.crc.com.cn/'},
        'longi': {'name': 'éš†åŸºè‚¡ä»½SRMç³»ç»Ÿ', 'url': 'https://srm.longi.com:6080'},
        'cgnpc': {'name': 'ä¸­å¹¿æ ¸ç”µå­å•†åŠ¡å¹³å°', 'url': 'https://ecp.cgnpc.com.cn'},
        'dongfang': {'name': 'ä¸œæ–¹ç”µæ°”', 'url': 'http://nsrm.dongfang.com/'},
        'zjycgzx': {'name': 'æµ™æ±Ÿäº‘é‡‡è´­ä¸­å¿ƒ', 'url': 'https://www.zjycgzx.com'},
        'ctg': {'name': 'ä¸­å›½ä¸‰å³¡ç”µå­é‡‡è´­å¹³å°', 'url': 'https://eps.ctg.com.cn/'},
        'sdicc': {'name': 'å›½æŠ•é›†å›¢ç”µå­é‡‡è´­å¹³å°', 'url': 'https://www.sdicc.com.cn/'},
        'csg': {'name': 'ä¸­å›½å—æ–¹ç”µç½‘ä¾›åº”é“¾æœåŠ¡å¹³å°', 'url': 'http://www.bidding.csg.cn/'},
        'sgccetp': {'name': 'å›½ç½‘ç”µå­å•†åŠ¡å¹³å°ç”µå·¥äº¤æ˜“ä¸“åŒº', 'url': 'https://sgccetp.com.cn/'},
        'powerbeijing': {'name': 'åŒ—äº¬äº¬èƒ½ç”µå­å•†åŠ¡å¹³å°', 'url': 'http://www.powerbeijing-ec.com'},
        'ccccltd': {'name': 'ä¸­äº¤é›†å›¢ä¾›åº”é“¾ç®¡ç†ç³»ç»Ÿ', 'url': 'http://ec.ccccltd.cn/'},
        'jchc': {'name': 'æ±Ÿè‹äº¤é€šæ§è‚¡', 'url': 'https://zbcg.jchc.cn/portal'},
        'minmetals': {'name': 'ä¸­å›½äº”çŸ¿é›†å›¢ä¾›åº”é“¾ç®¡ç†å¹³å°', 'url': 'https://ec.minmetals.com.cn/'},
        'sunwoda': {'name': 'æ¬£æ—ºè¾¾SRM', 'url': 'https://srm.sunwoda.com/'},
        'cnbm': {'name': 'ä¸­å›½å»ºæé›†å›¢é‡‡è´­å¹³å°', 'url': 'https://c.cnbm.com.cn/'},
        'hghn': {'name': 'åå…‰ç¯èƒ½æ•°å­—åŒ–é‡‡è´­ç®¡ç†å¹³å°', 'url': 'https://hgcg.hghngroup.com/'},
        'xcmg': {'name': 'å¾å·¥å…¨çƒæ•°å­—åŒ–ä¾›åº”é“¾ç³»ç»Ÿå¹³å°', 'url': 'http://xdsc.xcmg.com:8985/'},
        'xinecai': {'name': 'å®‰å¤©æ™ºé‡‡', 'url': 'http://www.xinecai.com'},
        'ariba': {'name': 'è¿œæ™¯SAPç³»ç»Ÿ', 'url': 'https://service.ariba.com/'},
        'faw': {'name': 'ä¸­å›½ä¸€æ±½ç”µå­æ‹›æ ‡é‡‡è´­äº¤æ˜“å¹³å°', 'url': 'https://srm.etp.faw.cn/staging'},
    }


class MonitorCore:
    """ç›‘æ§æ ¸å¿ƒç±»"""
    
    def __init__(self, 
                 keywords: List[str],
                 exclude_keywords: List[str] = None,
                 must_contain_keywords: List[str] = None,
                 notify_method: str = "email",
                 email: str = "",
                 phone: str = "",
                 email_config: Dict[str, Any] = None,
                 sms_config: Dict[str, Any] = None,
                 log_callback: Callable[[str], None] = None,
                 ai_config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç›‘æ§æ ¸å¿ƒ
        
        Args:
            keywords: æœç´¢å…³é”®å­—åˆ—è¡¨ (ORç»„ - è¡Œä¸šè¯)
            exclude_keywords: æ’é™¤å…³é”®å­—åˆ—è¡¨
            must_contain_keywords: å¿…é¡»åŒ…å«å…³é”®å­—åˆ—è¡¨ (ANDç»„ - äº§å“è¯)
            notify_method: é€šçŸ¥æ–¹å¼ (email/sms/both)
            email: é‚®ç®±åœ°å€
            phone: æ‰‹æœºå·
            email_config: é‚®ä»¶é…ç½®
            sms_config: çŸ­ä¿¡é…ç½®
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        """
        self.keywords = keywords
        self.exclude_keywords = exclude_keywords or []
        self.must_contain_keywords = must_contain_keywords or []
        self.notify_method = notify_method
        self.email = email
        self.phone = phone
        self.log_callback = log_callback or (lambda x: None)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.storage = Storage()
        self.matcher = KeywordMatcher(keywords, exclude_keywords, must_contain_keywords)
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self._load_config()
        
        # åˆå§‹åŒ–é€šçŸ¥å™¨
        if email_config:
            self.email_notifier = EmailNotifier(email_config)
        elif self.config.get('email'):
            email_cfg = self.config['email'].copy()
            if email:
                email_cfg['receiver'] = email
            self.email_notifier = EmailNotifier(email_cfg)
        else:
            self.email_notifier = None
        
        if sms_config:
            self.sms_notifier = SMSNotifier(sms_config)
        elif self.config.get('sms'):
            self.sms_notifier = SMSNotifier(self.config['sms'])
        else:
            self.sms_notifier = None
        
        # åˆå§‹åŒ– AI å®ˆå«
        self.ai_guard = None
        if ai_config and ai_config.get('enable'):
            try:
                from ai_guard import AIGuard
                self.ai_guard = AIGuard(ai_config, log_callback=self.log)
                self.log("âœ… [AI] æ™ºèƒ½è¿‡æ»¤å·²å¯ç”¨")
            except Exception as e:
                self.log(f"[WARN] AIåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–çˆ¬è™«
        self.crawlers = self._init_crawlers()
    
    def clear_data(self):
        """æ¸…ç©ºæ‰€æœ‰å†å²æ•°æ®"""
        self.storage.clear_all()
        self.log("All history data cleared.")

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        import yaml
        config_paths = [
            'config/config.yaml',
            '../config/config.yaml',
            os.path.join(os.path.dirname(__file__), '..', 'config', 'config.yaml')
        ]
        
        for path in config_paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f) or {}
        
        return {}
    
    def _init_crawlers(self) -> List:
        """åˆå§‹åŒ–æ‰€æœ‰çˆ¬è™«"""
        crawlers = []
        crawler_config = self.config.get('crawler', {})
        crawler_config['search_keywords'] = self.keywords[:3]
        
        # è·å–å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨
        enabled = crawler_config.get('enabled_sites', [])
        
        # 1. åŠ è½½å†…ç½®çˆ¬è™«ç±»
        crawler_classes = get_all_crawlers()
        
        for name in enabled:
            if name in crawler_classes:
                try:
                    crawler = crawler_classes[name](crawler_config)
                    crawlers.append(crawler)
                    self.log(f"[OK] Loaded crawler: {name}")
                except Exception as e:
                    self.log(f"[WARN] Failed to load crawler {name}: {e}")
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨Seleniumæ¨¡å¼
        use_selenium = crawler_config.get('use_selenium', False)
        self.log(f"[DEBUG] Seleniumæ¨¡å¼: {'å¯ç”¨' if use_selenium else 'ç¦ç”¨'}")
        
        # 2. åŠ è½½é»˜è®¤å†…ç½®ç½‘ç«™
        if use_selenium:
            try:
                from crawler.selenium_crawler import SeleniumCrawler, SELENIUM_AVAILABLE, IMPORT_ERROR_MSG
                self.log(f"[DEBUG] Seleniumå¯ç”¨: {SELENIUM_AVAILABLE}")
                if not SELENIUM_AVAILABLE:
                    self.log(f"[WARN] Seleniumæ¨¡å—å¯¼å…¥å¤±è´¥: {IMPORT_ERROR_MSG}ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼")
                    use_selenium = False
            except ImportError as e:
                self.log(f"[WARN] Seleniumæ¨¡å—æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼")
                use_selenium = False
        
        from crawler.custom import CustomCrawler
        default_sites = get_default_sites()
        
        for key in enabled:
            if key in default_sites and key not in crawler_classes:
                site = default_sites[key]
                try:
                    if use_selenium:
                        crawler = SeleniumCrawler(crawler_config, site['name'], site['url'], headless=True)
                        self.log(f"[OK] Loaded site (Selenium): {site['name']}")
                    else:
                        crawler = CustomCrawler(crawler_config, site['name'], site['url'])
                        self.log(f"[OK] Loaded site: {site['name']}")
                    crawlers.append(crawler)
                except Exception as e:
                    self.log(f"[WARN] Failed to load site {site['name']}: {e}")
        
        # 3. åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰çˆ¬è™«
        custom_sites = self.config.get('custom_sites', [])
        for site in custom_sites:
            try:
                name = site.get('name', 'Unknown')
                url = site.get('url', '')
                if name and url:
                    if use_selenium:
                        crawler = SeleniumCrawler(crawler_config, name, url, headless=True)
                        self.log(f"[OK] Loaded custom (Selenium): {name}")
                    else:
                        crawler = CustomCrawler(crawler_config, name, url)
                        self.log(f"[OK] Loaded custom crawler: {name}")
                    crawlers.append(crawler)
            except Exception as e:
                self.log(f"[WARN] Failed to load custom crawler {site.get('name')}: {e}")
        
        return crawlers
    
    def log(self, message: str):
        """è®°å½•æ—¥å¿—"""
        logging.info(message)
        self.log_callback(message)
    
    def run_once(self, progress_callback=None, stop_event=None) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸€æ¬¡ç›‘æ§
        
        Args:
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, site_name)
            stop_event: åœæ­¢äº‹ä»¶ï¼Œç”¨äºä¸­æ–­çˆ¬å–
        
        Returns:
            ç»“æœå­—å…¸ï¼ŒåŒ…å« new_count, failed_sites ç­‰
        """
        self.log("=" * 40)
        self.log(f"Start crawling at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        all_matched_bids = []
        failed_sites = []
        total_crawlers = len(self.crawlers)
        
        # AI è¿‡æ»¤ç»Ÿè®¡
        ai_stats = {
            'keyword_matched': [],  # å…³é”®è¯åŒ¹é…çš„é¡¹ç›® (title, url)
            'ai_approved': [],      # AI åˆ¤å®šç›¸å…³çš„é¡¹ç›® (title, url, reason)
            'ai_rejected': [],      # AI åˆ¤å®šä¸ç›¸å…³çš„é¡¹ç›® (title, url, reason)
        }
        
        for idx, crawler in enumerate(self.crawlers, 1):
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if stop_event and stop_event.is_set():
                self.log("æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ–­çˆ¬å–")
                break
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(idx, total_crawlers, crawler.name)
            
            try:
                self.log(f"Crawling: {crawler.name}...")
                bids = crawler.crawl(stop_event=stop_event)
                
                # çˆ¬å–åå†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
                if stop_event and stop_event.is_set():
                    self.log("æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ–­å¤„ç†")
                    break
                
                if bids is None:
                    # çˆ¬å–å¤±è´¥
                    failed_sites.append({
                        'name': crawler.name,
                        'error': 'Failed to fetch data (possibly blocked)'
                    })
                    self.log(f"[FAILED] {crawler.name}: Website may be blocking requests!")
                    continue
                
                # åŒ¹é…å…³é”®å­—
                matched_count = 0
                for bid in bids:
                    # åœ¨åŒ¹é…è¿‡ç¨‹ä¸­ä¹Ÿæ£€æŸ¥åœæ­¢ä¿¡å·
                    if stop_event and stop_event.is_set():
                        self.log("æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ–­åŒ¹é…")
                        break
                    
                    result = self.matcher.match_any(bid.title, bid.content)
                    
                    if result.matched:
                        # è®°å½•å…³é”®è¯åŒ¹é…çš„é¡¹ç›®
                        ai_stats['keyword_matched'].append({
                            'title': bid.title,
                            'url': bid.url
                        })
                        
                        # AI äºŒæ¬¡è¿‡æ»¤ (å¦‚æœå¯ç”¨)
                        if self.ai_guard:
                            ai_relevant, ai_reason = self.ai_guard.check_relevance(bid.title, bid.content or "")
                            if not ai_relevant:
                                ai_stats['ai_rejected'].append({
                                    'title': bid.title,
                                    'url': bid.url,
                                    'reason': ai_reason
                                })
                                self.log(f"[AIè¿‡æ»¤] è·³è¿‡: {bid.title[:30]}... (åŸå› : {ai_reason})")
                                continue
                            else:
                                ai_stats['ai_approved'].append({
                                    'title': bid.title,
                                    'url': bid.url,
                                    'reason': ai_reason
                                })
                        
                        if not self.storage.exists(bid):
                            self.storage.save(bid, notified=False)
                            all_matched_bids.append(bid)
                            matched_count += 1
                
                self.log(f"[OK] {crawler.name}: Found {len(bids)} items, {matched_count} new matches")
                
            except Exception as e:
                failed_sites.append({'name': crawler.name, 'error': str(e)})
                self.log(f"[ERROR] {crawler.name}: {e}")
        
        # å‘é€é€šçŸ¥
        if all_matched_bids:
            self.log(f"Sending notifications for {len(all_matched_bids)} new items...")
            self._send_notifications(all_matched_bids)
        else:
            self.log("No new matching items found")
        
        # æŠ¥å‘Šå¤±è´¥çš„ç½‘ç«™
        if failed_sites:
            self.log("-" * 40)
            self.log(f"WARNING: {len(failed_sites)} site(s) failed:")
            for site in failed_sites:
                self.log(f"  - {site['name']}: {site['error']}")
        
        # è¾“å‡º AI è¿‡æ»¤æ±‡æ€»æŠ¥å‘Š
        if self.ai_guard and (ai_stats['keyword_matched'] or ai_stats['ai_approved'] or ai_stats['ai_rejected']):
            self.log("")
            self.log("=" * 50)
            self.log("ğŸ“Š æœ¬æ¬¡æ£€ç´¢æ±‡æ€»æŠ¥å‘Š")
            self.log("=" * 50)
            self.log(f"ğŸ” å…³é”®è¯åŒ¹é…ç»“æœ: {len(ai_stats['keyword_matched'])} æ¡")
            
            if ai_stats['keyword_matched']:
                self.log("   åŒ¹é…çš„ç½‘é¡µé“¾æ¥:")
                for item in ai_stats['keyword_matched'][:10]:  # æœ€å¤šæ˜¾ç¤º10æ¡
                    self.log(f"   â€¢ {item['title'][:40]}...")
                    self.log(f"     {item['url']}")
                if len(ai_stats['keyword_matched']) > 10:
                    self.log(f"   ... è¿˜æœ‰ {len(ai_stats['keyword_matched']) - 10} æ¡")
            
            self.log("")
            self.log(f"âœ… AIåˆ¤æ–­ç¬¦åˆè¦æ±‚: {len(ai_stats['ai_approved'])} æ¡")
            if ai_stats['ai_approved']:
                for item in ai_stats['ai_approved'][:5]:
                    self.log(f"   âœ“ {item['title'][:35]}... (ç†ç”±: {item['reason']})")
            
            self.log("")
            self.log(f"âŒ AIåˆ¤æ–­ä¸ç¬¦åˆ: {len(ai_stats['ai_rejected'])} æ¡")
            if ai_stats['ai_rejected']:
                for item in ai_stats['ai_rejected'][:5]:
                    self.log(f"   âœ— {item['title'][:35]}... (ç†ç”±: {item['reason']})")
            
            self.log("=" * 50)
        
        self.log("=" * 40)
        
        # å…³é—­å…±äº«æµè§ˆå™¨ä»¥é‡Šæ”¾å†…å­˜
        try:
            from crawler.selenium_crawler import SharedBrowserManager
            SharedBrowserManager.close()
            self.log("âœ… å·²å…³é—­å…±äº«æµè§ˆå™¨ï¼Œé‡Šæ”¾å†…å­˜")
        except:
            pass
        
        return {
            'new_count': len(all_matched_bids),
            'failed_sites': failed_sites,
            'total_crawlers': len(self.crawlers),
            'ai_stats': ai_stats
        }
    
    def _send_notifications(self, bids: List[BidInfo]):
        """å‘é€é€šçŸ¥"""
        success = False
        
        if self.notify_method in ('email', 'both') and self.email_notifier:
            try:
                if self.email:
                    # ä¸´æ—¶ä¿®æ”¹æ”¶ä»¶äºº
                    original_receiver = self.email_notifier.receiver
                    self.email_notifier.receiver = self.email
                    success = self.email_notifier.send(bids)
                    self.email_notifier.receiver = original_receiver
                else:
                    success = self.email_notifier.send(bids)
                
                if success:
                    self.log(f"[OK] Email sent to {self.email or self.email_notifier.receiver}")
            except Exception as e:
                self.log(f"[ERROR] Email failed: {e}")
        
        if self.notify_method in ('sms', 'both') and self.sms_notifier:
            try:
                if self.phone:
                    success = self.sms_notifier.send(self.phone, bids)
                    if success:
                        self.log(f"[OK] SMS sent to {self.phone}")
            except Exception as e:
                self.log(f"[ERROR] SMS failed: {e}")
        
        if success:
            for bid in bids:
                self.storage.mark_notified(bid)
